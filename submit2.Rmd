---
title: "Movielens Ratings Prediction"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

## 1. Dataset description and project goals

This data set contains 10000054 ratings and 95580 tags applied to 10681 movies by 71567 users of the online movie recommender service MovieLens.

Users were selected at random for inclusion. All users selected had rated at least 20 movies. Unlike other MovieLens data sets, no demographic information is included. Each user is represented by an id, and no other information is provided. The data are contained in three files, movies.dat, ratings.dat and tags.dat, of which we'll use only the former two. This and other GroupLens data sets are publicly available for download at GroupLens Data Sets.

The goal of the project is to generate ratings prediction with the minimum RMSE and maximum accuracy. Spercifically, the task requires RMSE to be lower (i.e. better) than 0.87750 in order to obtain maximum points.

## 2. Ingesting and exploring the data

First, let's download relevant libraries:
```{r}
suppressMessages(if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org"))
suppressMessages(if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org"))
suppressMessages(if(!require(tm)) install.packages("tm", repos = "http://cran.us.r-project.org"))
suppressMessages(if(!require(slam)) install.packages("slam", repos = "http://cran.us.r-project.org"))
suppressMessages(if(!require(igraph)) install.packages("igraph", repos = "http://cran.us.r-project.org"))
suppressMessages(if(!require(Matrix)) install.packages("Matrix", repos = "http://cran.us.r-project.org"))
suppressMessages(if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org"))
suppressMessages(if(!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org"))
suppressMessages(if(!require(SparseM)) install.packages("SparseM", repos = "http://cran.us.r-project.org"))
suppressMessages(if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org"))
```
If the data are not on your hard drive, use the following to download it and create a movielens dataframe for future analysis, including adjusting some of the feature types (commented out since I have the data on my hard drive; uncomment if you need to run):
```{r}
# dl <- tempfile()
# download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
# ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
#                       col.names = c("userId", "movieId", "rating", "timestamp"))
# movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
# colnames(movies) <- c("movieId", "title", "genres")
# movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                            genres = as.character(genres))
# movielens <- left_join(ratings, movies, by = "movieId")
```
If the data is on your hard drive, replace the above cell to this one and use your computer's path:
```{r}
ratings <- read.table(text = gsub("::", "\t", readLines("ml-10M100K/ratings.dat")),
                      col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines("ml-10M100K/movies.dat"), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
```
Let's set the validation portion to be 10% of movielens data:
```{r}
set.seed(46)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
train <- movielens[-test_index,]
temp <- movielens[test_index,]
```
Make sure userId and movieId in validation set are also in train set, and add rows removed from validation set back into train set:
```{r}
validation <- temp %>% 
     semi_join(train, by = "movieId") %>%
     semi_join(train, by = "userId")
removed <- anti_join(temp, validation)
train <- rbind(train, removed)
```
Let's check the number of genres:
```{r}
length(unique(movielens$genres))
head(unique(movielens$genres))
```
Multiple genres are joined together creating a list of combined "genres", which does not make sense in identifying distinct genres, so need to distill:
```{r}
corp <- VCorpus(VectorSource(movies$genres))
dtm <- DocumentTermMatrix(corp, 
                          control = list(tokenize = function(x) 
                            unlist(strsplit(as.character(x), "\\|"))))
dtm$dimnames$Terms
```
Need to remove "(no genres listed)" as it is not useful for the model:
```{r}
corp1 <- tm_map(corp, removeWords, "(no genres listed)")
dtm1 <- DocumentTermMatrix(corp1, 
                          control = list(tokenize = function(x) 
                            unlist(strsplit(as.character(x), "\\|"))))
dtm1$dimnames$Terms
```
We can use these genre names later as additional features if we can not reach the required RMSE.

Let's understand what genres are considered close to each other, create matrix to be used for adjacency analysis:
```{r}
adj <- crossprod_simple_triplet_matrix(dtm1); adj
```
We can see that e.g. action is considered relatively close to thriller and adventure.  A graph can help us depict the adjacency of different genres better:
```{r, fig.height=8}
plot(g <- graph_from_adjacency_matrix(adj, mode="undirected", weighted=TRUE))
```
We can see how close various genres are to each other which will influence our predictions.  E.g. film-noir appears to be category of its own, with the closest (yet quite distant relative to most others) genres being mystery and crime, meanwhile action genre seems to be closely related to thriller, comedy, and adventure.

Also, the data appears to be rather sparse: 
```{r}
UMRmatrix <- sparseMatrix(i = movielens$userId,
                         j = movielens$movieId,
                         x = movielens$rating)
print(UMRmatrix)
```
An image of a small sample will help us visualize the sparsity:
```{r}
#image of a subset of 100 users and 200 movies
image(UMRmatrix[1:100,1:200], xlab="Movies", ylab="Users", lwd=3, border.col = adjustcolor("skyblue", 1/2))
```
What this implies is that some movies get rated more than others...
```{r}
movielens %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "blue", alpha = 0.5) + 
  scale_x_log10() + 
  ggtitle("Movies")
```
...and some users are more active in rating the movies than other:
```{r}
movielens %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "forestgreen", alpha = 0.5) + 
  scale_x_log10() + 
  ggtitle("Users")
```
In essence, this is what you'd expect in any ratings database, unless there are tangible incentives to enter ratings (should decrease sparsity) or the ratings are required (should reduce or eliminate sparsity depending on the requirement's enforcement).

Let's also look at how the ratings are distributed (and use "b" following statistics convention for denoting "effect"):
```{r}
train %>% 
  group_by(userId) %>% 
  summarize(b_usr = mean(rating)) %>% 
  ggplot(aes(b_usr)) + 
  geom_histogram(bins = 30, color = "black", fill = "forestgreen", alpha = 0.5)
```
How about if we count only those users who rated more than 50 movies?
```{r}
train %>% 
  group_by(userId) %>% 
  summarize(b_usr = mean(rating)) %>% 
  filter(n()>=50) %>%
  ggplot(aes(b_usr)) + 
  geom_histogram(bins = 30, color = "black", fill = "forestgreen", alpha = 0.5)
```
What about the users who rated more than 100 movies?
```{r}
train %>% 
  group_by(userId) %>% 
  summarize(b_usr = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_usr)) + 
  geom_histogram(bins = 30, color = "black", fill = "forestgreen", alpha = 0.5)
```
The distributions are largely similar.

## 3. Analysis and Results

Let's move into the analysis phase and start by defining RMSE which we'll use to measure the accuracy of predicted versus actual ratings:
```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}
```
What is the average rating for the training set?
```{r}
mu <- mean(train$rating); mu
```
Let's see what RMSE we would get by using the average rating for predicted ratings in the validation set, and use it as a benchmark for further methods (unless magically we get an RMSE better than the required 0.8775):
```{r}
naive_rmse <- RMSE(validation$rating, mu)
rmse_results <- data_frame(method = "Simple average", RMSE = naive_rmse); rmse_results
```
I an unable to run a linear regression with userId and movieId as factors, since my computer's vector memory cannot handle the size.  Here, I would like to use Gradient Boosting Machine (GBM), but due to the size of the data and my computer's limited power, I'll try XGB (Extreme Gradient Boosting) which is much faster than GBM:
```{r}
#XGB requires matrix data, so need to convert the dataframe into matrix before applying the method
set.seed(46)
train_x <- subset(train, select = c(userId, movieId, rating))
train_x <- setDT(train_x)
train_x <- data.table(train_x, keep.rownames = F)
rating <- train_x$rating
new_tr <- model.matrix(~.+0, data = train_x[,-c("rating"), with=F])
train_mx <- xgb.DMatrix(data = new_tr,label = rating) 
bst <- xgboost(data = train_mx, label = rating, max.depth = 7, eta = 0.4, nrounds = 40,
               nthread = 2, lambda = 0.7, gamma = 1, objective = "reg:linear")
```
Let's check RMSE for the predictions:
```{r}
validation_x <- subset(validation, select = c(userId, movieId, rating))
validation_x <- setDT(validation_x)
validation_x <- data.table(validation_x, keep.rownames = F)
rating <- validation_x$rating
new_val <- model.matrix(~.+0, data = validation_x[,-c("rating"), with=F])
validation_mx <- xgb.DMatrix(data = new_val,label = rating) 
xgbpred <- predict(bst,validation_mx)
xgb1_pred <- RMSE(rating, xgbpred)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="XGB 1",  
                          RMSE = xgb1_pred))
rmse_results %>% knitr::kable()
```
Improvement in RMSE is significant, but insufficient.  Let's calculate movie and user effects as the next step, and re-apply XGB just using those effects:
```{r}
mu <- mean(train$rating)
mov_eff <- train %>% 
  group_by(movieId) %>% 
  summarize(mov_eff = mean(rating - mu))
usr_eff <- train %>% 
  left_join(mov_eff, by='movieId') %>%
  group_by(userId) %>%
  summarize(usr_eff = mean(rating - mu - mov_eff))
```
Let's create a dataframe which will add respective effects to direct movie and user ratings:
```{r}
train_eff <- 
    train %>% 
    left_join(mov_eff, by = "movieId") %>%
    left_join(usr_eff, by = "userId")
validation_eff <- 
    validation %>% 
    left_join(mov_eff, by = "movieId") %>%
    left_join(usr_eff, by = "userId")
```
Select the effect features, convert the resulting dataframe into matrix, and run XGB #2:
```{r}
set.seed(46)
train_xf <- subset(train_eff, select = c(mov_eff, usr_eff, rating))
train_xf <- setDT(train_xf)
train_xf <- data.table(train_xf, keep.rownames = F)
ratingf <- train_xf$rating
new_trf <- model.matrix(~.+0, data = train_xf[,-c("rating"), with=F])
train_mxf <- xgb.DMatrix(data = new_trf,label = ratingf) 
bstf <- xgboost(data = train_mxf, label = ratingf, max.depth = 7, eta = 0.4, nrounds = 40,
               nthread = 2, lambda = 0.7, gamma = 1, objective = "reg:linear")
```
Let's check RMSE for the new XGB model:
```{r}
validation_xf <- subset(validation_eff, select = c(mov_eff, usr_eff, rating))
validation_xf <- setDT(validation_xf)
validation_xf <- data.table(validation_xf, keep.rownames = F)
ratingf <- validation_xf$rating
new_valf <- model.matrix(~.+0, data = validation_xf[,-c("rating"), with=F])
validation_mxf <- xgb.DMatrix(data = new_valf,label = ratingf) 
xgbpredf <- predict(bstf,validation_mxf)
xgb2_pred <- RMSE(ratingf, xgbpredf)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="XGB 2",  
                          RMSE = xgb2_pred))
rmse_results %>% knitr::kable()
```

## 4. Conclusion

Using Extreme Gradient Boosting (XGB) algorithm on movie and uaer effect values, we were able to achieve RMSE of 0.86, which is better than the required 0.8775.

